# Language Models Are Implicitly Continuous

Code for the ICLR 2025 paper ["Language Models Are Implicitly Continuous"](https://arxiv.org/abs/2504.03933) (Marro, Evangelista, Huang, La Malfa, Lombardi, Wooldridge).

This is a fork of [huggingface/transformers](https://github.com/huggingface/transformers) to study the behaviour of continuous LLMs.

# Running

For a quick start, see `continuity_examples/main.ipynb`.

We also have scripts in `continuity_scripts`.
